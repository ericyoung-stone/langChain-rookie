{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to stream runnables\n",
    "[How to stream runnables](https://python.langchain.com/docs/how_to/streaming/) 如何流式处理可运行程序\n",
    "- LangChain primitives like **chat models, output parsers, prompts, retrievers, and agents** implement the **LangChain Runnable Interface.**\n",
    "- 提供了两种一般方法来流式传输内容:\n",
    "    - sync stream and async astream, (同步 stream 和 异步 astream ：默认的流式传输实现，从链中流式**传输最终输出**。)\n",
    "    - async astream_events and async astream_log, (异步 astream_events 和 异步 astream_log ：这些提供了从链中流式传输**中间步骤**和**最终输出**的方法。)"
   ],
   "id": "89facdfe5398a5f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## chat models",
   "id": "8270284de4034745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:47:37.705826Z",
     "start_time": "2025-03-16T04:47:37.678677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "from prompt_toolkit.key_binding.bindings.named_commands import complete\n",
    "\n",
    "sys.path.append('/Users/ericyoung/ysx/code/github-study/langChain-rookie')\n",
    "from env_utils import load_environment_variables\n",
    "\n",
    "load_environment_variables()"
   ],
   "id": "4690e4a032256990",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__file__: /Users/ericyoung/ysx/code/github-study/langChain-rookie/env_utils.py\n",
      "dotenv_path1: /Users/ericyoung/ysx/code/github-study/langChain-rookie/.env\n",
      "load env ok\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 支持tool的 chat model",
   "id": "6820e42336581aeb"
  },
  {
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-03-16T04:47:38.333688Z",
     "start_time": "2025-03-16T04:47:37.773222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "# llm = ChatOllama(\n",
    "#     model=\"qwen2.5:latest\", # gemma3:latest(不支持tool)\n",
    "#     temperature=0.7,\n",
    "#     # other params...\n",
    "# )"
   ],
   "id": "587fe31e8dc504d6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 不支持tool的chat model",
   "id": "b8cdb0a145f350a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:47:44.987828Z",
     "start_time": "2025-03-16T04:47:38.340362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 任何chat model\n",
    "from models import MyOpenAIModel\n",
    "\n",
    "model = MyOpenAIModel()\n",
    "response = model.generate(\"你好，介绍一下你自己\")\n",
    "print(response)"
   ],
   "id": "69a54d1249199c66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！我是一个大型语言模型，由 Google 训练。\n",
      "\n",
      "我可以：\n",
      "\n",
      "*   **生成不同类型的文本格式:** 例如诗歌、代码、剧本、音乐作品、电子邮件、信件等。我会尽力满足您的所有要求。\n",
      "*   **回答您的各种问题:** 无论您是想了解事实、需要一些建议，还是仅仅想闲聊，我都会尽力提供帮助。\n",
      "*   **进行翻译:** 我可以将文本从一种语言翻译成另一种语言。\n",
      "*   **总结文本:**  我可以将较长的文章或文档总结成更简洁的版本。\n",
      "\n",
      "我还在不断学习和改进中，我的知识截止到 2023 年 4 月。\n",
      "\n",
      "您想和我聊些什么呢？ 或者您有什么具体的问题想要问我吗？ 例如，您可以问：\n",
      "\n",
      "*   “请写一首关于秋天的诗。”\n",
      "*   “什么是人工智能？”\n",
      "*   “如何制作一个简单的蛋糕？”您好！我是一个大型语言模型，由 Google 训练。\n",
      "\n",
      "我可以：\n",
      "\n",
      "*   **生成不同类型的文本格式:** 例如诗歌、代码、剧本、音乐作品、电子邮件、信件等。我会尽力满足您的所有要求。\n",
      "*   **回答您的各种问题:** 无论您是想了解事实、需要一些建议，还是仅仅想闲聊，我都会尽力提供帮助。\n",
      "*   **进行翻译:** 我可以将文本从一种语言翻译成另一种语言。\n",
      "*   **总结文本:**  我可以将较长的文章或文档总结成更简洁的版本。\n",
      "\n",
      "我还在不断学习和改进中，我的知识截止到 2023 年 4 月。\n",
      "\n",
      "您想和我聊些什么呢？ 或者您有什么具体的问题想要问我吗？ 例如，您可以问：\n",
      "\n",
      "*   “请写一首关于秋天的诗。”\n",
      "*   “什么是人工智能？”\n",
      "*   “如何制作一个简单的蛋糕？”\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ⭐️ 不支持tool模型 利用 ChatOpenAI (使用base_url) 实现 bind_tools\n",
    "- 支持结构化输出, with_structured_output方法\n",
    "- 支持使用工具, bind_tools方法\n",
    "- llm = ChatOpenAI(\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "                # api_key=\"...\",\n",
    "                # base_url=\"...\",\n",
    "                # organization=\"...\",\n",
    "                # other params...\n",
    "            )"
   ],
   "id": "f0010a0917804b50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:47:46.839077Z",
     "start_time": "2025-03-16T04:47:45.068147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models import get_base_url_model_with_tools\n",
    "llm = get_base_url_model_with_tools()\n",
    "print(llm.invoke(\"你是谁?\").content)"
   ],
   "id": "70c8cc470568602c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是Gemma，一个开放权重的AI助手。我是一个由Google DeepMind训练的大型语言模型。\n",
      "\n",
      "作为开源模型，我可以被广泛使用和修改。 \n",
      "\n",
      "你可以通过访问我的界面与我互动。\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 使用 Stream\n",
    "- 所有的 Runnable 对象都实现了一个名为 stream 的方法，以及一个异步 astream 方法\n",
    "- 这些方法设计用于分块流式传输最终输出，在每个块可用后立即生成每个块。"
   ],
   "id": "a759e3db933e3003"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### LLMs and Chat Models\n",
    "- LLMs和聊天模型\n",
    "- 首先，我们可以使用同步 stream API："
   ],
   "id": "f10f733263589951"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:56:16.911869Z",
     "start_time": "2025-03-16T04:56:08.181134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks = []\n",
    "for chunk in llm.stream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "complete_str = ''.join([c.content for c in chunks ])\n",
    "print(complete_str)"
   ],
   "id": "ddea575b877cd02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This| is| a| classic| question| that| has| surprisingly| complex| answers|!| The| sky| isn|'|t| actually| *|one|*| color|.| Here|’|s| the| breakdown|:|\n",
      "\n",
      "|*| **|During| the| day| (|when| it|'|s| bright| blue|):|**| The| sky| appears| blue| because| of| a| phenomenon| called| **|Ray|leigh| scattering|**.| Sunlight| is| made| up| of| all| colors|,| and| when| it| enters| Earth|’|s| atmosphere|,| it| bumps| into| air| molecules| (|mostly| nitrogen| and| oxygen|).| Blue| light| has| shorter| wavelengths| and| is| scattered| much| more| than| other| colors| like| red| or| orange|.| This| scattered| blue| light| reaches| our| eyes| from| all| directions|,| making| the| sky| appear| blue|.|\n",
      "\n",
      "|*| **|At| sunrise| and| sunset|:**| The| sky| appears| reddish| or| orange| because| when| the| sun| is| low| on| the| horizon|,| sunlight| has| to| travel| through| *|much|*| more| of| the| atmosphere|.|  |The| blue| light| gets| scattered| away| almost| completely| before| it| can| reach| us|.| This| leaves| the| longer| wavelengths| –| red|,| orange|,| and| yellow| –| which| are| less| affected| by| scattering|,| so| they| dominate| what| we| see|.|\n",
      "\n",
      "|*| **|At| night|:**| The| sky| is| black| because| there|'|s| no| direct| sunlight| to| be| scattered|.|\n",
      "\n",
      "\n",
      "|**|So|,| the| short| answer| is|:|  |The| sky| appears| blue| during| the| day| due| to| scattering|,| and| can| appear| red|/|orange| at| sunrise|/|sunset| or| black| at| night|.**|\n",
      "\n",
      "\n",
      "\n",
      "|Do| you| want| me| to| delve| into| any| of| these| aspects| in| more| detail| (|like| explaining| Rayleigh| scattering| further|)?||This is a classic question that has surprisingly complex answers! The sky isn't actually *one* color. Here’s the breakdown:\n",
      "\n",
      "* **During the day (when it's bright blue):** The sky appears blue because of a phenomenon called **Rayleigh scattering**. Sunlight is made up of all colors, and when it enters Earth’s atmosphere, it bumps into air molecules (mostly nitrogen and oxygen). Blue light has shorter wavelengths and is scattered much more than other colors like red or orange. This scattered blue light reaches our eyes from all directions, making the sky appear blue.\n",
      "\n",
      "* **At sunrise and sunset:** The sky appears reddish or orange because when the sun is low on the horizon, sunlight has to travel through *much* more of the atmosphere.  The blue light gets scattered away almost completely before it can reach us. This leaves the longer wavelengths – red, orange, and yellow – which are less affected by scattering, so they dominate what we see.\n",
      "\n",
      "* **At night:** The sky is black because there's no direct sunlight to be scattered.\n",
      "\n",
      "\n",
      "**So, the short answer is:  The sky appears blue during the day due to scattering, and can appear red/orange at sunrise/sunset or black at night.**\n",
      "\n",
      "\n",
      "\n",
      "Do you want me to delve into any of these aspects in more detail (like explaining Rayleigh scattering further)?\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 如果您在异步环境中工作，可以考虑使用异步 astream API：",
   "id": "d3862df336c10fa0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T04:56:36.718378Z",
     "start_time": "2025-03-16T04:56:27.810378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunks = []\n",
    "async for chunk in llm.astream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "complete_str = ''.join([c.content for c in chunks ])\n",
    "print(complete_str)"
   ],
   "id": "4c2fefbb77a00252",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That|'|s| a| fantastic| question|,| and| it|’|s| surprisingly| complex|!| The| sky| isn|’|t| *|always|*| one| single| color|.| Here|’|s| a| breakdown|:|\n",
      "\n",
      "|*| **|During| the| day| (|usually|):|**| Most| of| the| time|,| the| sky| appears| **|blue|**.| This| is| due| to| a| phenomenon| called| Rayleigh| scattering|.| Sunlight| is| made| up| of| all| colors|,| but| blue| light| has| shorter| wavelengths| and| scat|ters| more| easily| in| the| atmosphere| than| other| colors| like| red| or| orange|.| Think| of| it| like| tiny| air| molecules| bouncing| blue| light| around| in| every| direction|.|\n",
      "\n",
      "|*| **|Sunrise| &| Sunset|:**| When| the| sun| is| low| on| the| horizon|,| sunlight| has| to| travel| through| *|much|*| more| of| the| atmosphere| to| reach| our| eyes|.|  |This| means| almost| all| of| the| blue| light| gets| scattered| away| before| it| reaches| us|.| The| longer| wavelengths| –| red|,| orange|,| and| yellow| –| are| able| to| pass| through| more| easily|,| creating| those| beautiful| colorful| sunsets| and| sun|r|ises|.|\n",
      "\n",
      "|*| **|Other| colors|:**| Sometimes| you| can| see| other| colors| in| the| sky| -| grey| (|when| cloudy|),| pink| (|during| certain| sunset| conditions|),| or| even| green| (|rare|ly|,| due| to| specific| atmospheric| conditions|).|\n",
      "\n",
      "\n",
      "|**|So|,| to| answer| your| question| simply|:| The| sky| is| *|usually|*| blue|.**| |\n",
      "\n",
      "|Do| you| want| me| to| delve| deeper| into| any| of| these| aspects|,| like| Rayleigh| scattering| or| why| sunsets| are| colorful|?||That's a fantastic question, and it’s surprisingly complex! The sky isn’t *always* one single color. Here’s a breakdown:\n",
      "\n",
      "* **During the day (usually):** Most of the time, the sky appears **blue**. This is due to a phenomenon called Rayleigh scattering. Sunlight is made up of all colors, but blue light has shorter wavelengths and scatters more easily in the atmosphere than other colors like red or orange. Think of it like tiny air molecules bouncing blue light around in every direction.\n",
      "\n",
      "* **Sunrise & Sunset:** When the sun is low on the horizon, sunlight has to travel through *much* more of the atmosphere to reach our eyes.  This means almost all of the blue light gets scattered away before it reaches us. The longer wavelengths – red, orange, and yellow – are able to pass through more easily, creating those beautiful colorful sunsets and sunrises.\n",
      "\n",
      "* **Other colors:** Sometimes you can see other colors in the sky - grey (when cloudy), pink (during certain sunset conditions), or even green (rarely, due to specific atmospheric conditions).\n",
      "\n",
      "\n",
      "**So, to answer your question simply: The sky is *usually* blue.** \n",
      "\n",
      "Do you want me to delve deeper into any of these aspects, like Rayleigh scattering or why sunsets are colorful?\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chains 链条\n",
    "- 该链路结合了提示、模型和解析器，并验证流式传输是否正常工作。"
   ],
   "id": "29f652e94d015268"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T12:14:45.921874Z",
     "start_time": "2025-03-16T12:14:43.310863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ],
   "id": "85f07681212be9c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why| did| the| parrot| cross| the| playground|?| |\n",
      "\n",
      "|To| get| to| the| other| slide|!| 😂| |\n",
      "\n",
      "|---|\n",
      "\n",
      "|Would| you| like| to| hear| another| joke|?| 😊||"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 虽然我们在上述链子的末尾使用了 parser ，但我们仍然得到了流式输出。\n",
    "- parser 会在每个流式数据块上独立操作。\n",
    "- 许多 LCEL 原语也支持这种类型的转换式流式处理，这对于构建应用程序非常方便。"
   ],
   "id": "92a3c01f8f34493a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Working with Input Streams 处理输入流\n",
    "- 如果你想在生成时流式传输JSON输出怎么办？\n",
    "    - 如果依赖 json.loads 解析部分 json，解析将会失败，因为部分 json 不是有效的 json\n",
    "- 实际上有一种方法可以做到这一点: **解析器需要在输入流上操作**，并尝试“自动补全”部分 json 使其处于有效状态。"
   ],
   "id": "6c15109ec5cc541e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T12:19:57.623211Z",
     "start_time": "2025-03-16T12:19:54.074506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    llm | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\" ):\n",
    "    print(text, flush=True)"
   ],
   "id": "bbd9c954cab4ba02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 6}]}\n",
      "{'countries': [{'name': 'France', 'population': 67}]}\n",
      "{'countries': [{'name': 'France', 'population': 678}]}\n",
      "{'countries': [{'name': 'France', 'population': 6789}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890}]}\n",
      "{'countries': [{'name': 'France', 'population': 678905}]}\n",
      "{'countries': [{'name': 'France', 'population': 6789053}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 4}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 477}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 4775}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 477525}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 4775256}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 1}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 12}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 125}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 1257}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 12571}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 125710}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 1257100}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 12571000}]}\n",
      "{'countries': [{'name': 'France', 'population': 67890534}, {'name': 'Spain', 'population': 47752566}, {'name': 'Japan', 'population': 125710000}]}\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 现在，让我们拆解流式处理。\n",
    "- 我们将使用之前的例子，并在末尾附加一个提取函数，从最终的 JSON 中提取国家名称。\n",
    "- 此时由于增加了提取函数, 导致输出就不是流式的了, 见后,可通过生成起函数(yield) 实现流式"
   ],
   "id": "3c71f7c26651517d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T12:23:03.188491Z",
     "start_time": "2025-03-16T12:22:59.581334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import (\n",
    "    JsonOutputParser,\n",
    ")\n",
    "\n",
    "\n",
    "# A function that operates on finalized inputs\n",
    "# rather than on an input_stream\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = llm | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ],
   "id": "9c28adff234fa0e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### ⭐️Generator Functions 生成器函数助理流式输出\n",
    "- 使用一个可以操作输入流的生成器函数来**修复流式传输**。\n",
    "- 一个生成器函数（一个使用 yield 的函数）允许编写处理输入流的代码"
   ],
   "id": "247916dee830cdcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T12:30:45.960177Z",
     "start_time": "2025-03-16T12:30:42.410294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "async def _extract_country_names_streaming(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = llm | JsonOutputParser() | _extract_country_names_streaming\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ],
   "id": "917509f72088d8cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Non-streaming components 非流式组件\n",
    "- 并不是所有的组件都需要实现流处理——在某些情况下，流处理可能是不必要的、难以实现的，或者根本没有意义。"
   ],
   "id": "e0bc0e54f5204fea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T13:54:08.647679Z",
     "start_time": "2025-03-16T13:54:08.113727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from embeddings import LMStudioEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "embedding = LMStudioEmbeddings()\n",
    "vectorstore = FAISS.from_texts([\"harrison worked at kensho\", \"harrison likes spicy food\"], embedding=embedding)"
   ],
   "id": "ddd6446085718124",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T13:54:10.172270Z",
     "start_time": "2025-03-16T13:54:10.043259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ],
   "id": "3a4a7ab742018399",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='4e5c7990-2419-48b9-99a7-3a695020e948', metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(id='89c2440e-8b25-4a2a-986e-d6ee832cb35b', metadata={}, page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 使用非流处理组件构建的 LCEL 链，在很多情况下仍然能够流处理，流处理的部分输出将在链中的最后一个非流处理步骤之后开始。",
   "id": "6d8d7e63cccf279b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T13:54:26.766216Z",
     "start_time": "2025-03-16T13:54:21.867458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in retrieval_chain.stream(\n",
    "    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"\n",
    "):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ],
   "id": "336ad9c40c05d990",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based| on| the| provided| documents|,| Harrison| worked| at| K|ens|ho|.| Here| are| three| made|-|up| sentences| about| K|ens|ho|:|\n",
      "\n",
      "|1|.|  |K|ens|ho|’|s| sleek| offices| offered| stunning| views| of| the| city| skyline|,| perfect| for| brainstorming| innovative| data| solutions|.|\n",
      "|2|.|  |The| team| at| K|ens|ho| was| known| for| its| collaborative| spirit| and| dedication| to| pushing| the| boundaries| of| artificial| intelligence|.|\n",
      "|3|.|  |K|ens|ho|'|s| state|-|of|-|the|-|art| servers| hum|med| with| complex| algorithms|,| powering| groundbreaking| research| in| natural| language| processing|.||"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ??Using Stream Events 使用事件流\n",
    "- 此指南演示了 V2 API，并要求 langchain-core >= 0.2。对于与 LangChain 较旧版本兼容的 V1 API，请参阅[此处](https://python.langchain.com/v0.1/docs/expression_language/streaming/#using-stream-events)"
   ],
   "id": "2a55fa9c6a5129e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:01:53.467990Z",
     "start_time": "2025-03-16T14:01:53.460354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import langchain_core\n",
    "\n",
    "langchain_core.__version__"
   ],
   "id": "fbf7cb94446d7bb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.37'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 尽可能在整个代码中使用 async （例如，异步工具等）\n",
    "- 在不使用 LCEL 的情况下使用可运行任务时，请确保调用 .astream() 在LLMs而不是 .ainvoke 以强制LLM流式传输令牌。"
   ],
   "id": "dc6b0036ac55ee14"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chat Model\n",
    "- 对于 langchain-core<0.3.37 ，请显式设置 version 关键字（例如， model.astream_events(\"hello\", version=\"v2\") ）"
   ],
   "id": "39bca6de4a241095"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:05:44.716824Z",
     "start_time": "2025-03-16T14:05:42.321501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "events = []\n",
    "async for event in llm.astream_events(\"hello\", version=\"v2\"):\n",
    "    events.append(event)"
   ],
   "id": "d00ad0eff7c6760",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:05:56.695258Z",
     "start_time": "2025-03-16T14:05:56.689349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 让我们看看一些开始事件和一些结束事件。\n",
    "events[:3]"
   ],
   "id": "5d23ed6b3419cd1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_start',\n",
       "  'data': {'input': 'hello'},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'run_id': 'a91051d7-de44-4746-ab5b-116d678b8df3',\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gemma-3-4b-it',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7,\n",
       "   'ls_max_tokens': 8064},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': 'a91051d7-de44-4746-ab5b-116d678b8df3',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gemma-3-4b-it',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7,\n",
       "   'ls_max_tokens': 8064},\n",
       "  'data': {'chunk': AIMessageChunk(content='Hello', additional_kwargs={}, response_metadata={}, id='run-a91051d7-de44-4746-ab5b-116d678b8df3')},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': 'a91051d7-de44-4746-ab5b-116d678b8df3',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gemma-3-4b-it',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7,\n",
       "   'ls_max_tokens': 8064},\n",
       "  'data': {'chunk': AIMessageChunk(content=' there', additional_kwargs={}, response_metadata={}, id='run-a91051d7-de44-4746-ab5b-116d678b8df3')},\n",
       "  'parent_ids': []}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:07:32.924220Z",
     "start_time": "2025-03-16T14:07:32.916916Z"
    }
   },
   "cell_type": "code",
   "source": "events[-2:]",
   "id": "a479f2d3303656fb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_stream',\n",
       "  'run_id': 'a91051d7-de44-4746-ab5b-116d678b8df3',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gemma-3-4b-it',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7,\n",
       "   'ls_max_tokens': 8064},\n",
       "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gemma-3-4b-it', 'system_fingerprint': 'gemma-3-4b-it'}, id='run-a91051d7-de44-4746-ab5b-116d678b8df3')},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_end',\n",
       "  'data': {'output': AIMessageChunk(content=\"Hello there! How's your day going so far? Is there anything I can help you with today, or were you just saying hello? 😊 \\n\\nDo you want to:\\n\\n*   Chat about something?\\n*   Ask me a question?\\n*   Play a game?\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gemma-3-4b-it', 'system_fingerprint': 'gemma-3-4b-it'}, id='run-a91051d7-de44-4746-ab5b-116d678b8df3')},\n",
       "  'run_id': 'a91051d7-de44-4746-ab5b-116d678b8df3',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gemma-3-4b-it',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7,\n",
       "   'ls_max_tokens': 8064},\n",
       "  'parent_ids': []}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Chain\n",
    "- 重新审视一下解析流式 JSON 的示例链，以探索流式事件 API。"
   ],
   "id": "9f37fe5215bba0a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:09:26.120967Z",
     "start_time": "2025-03-16T14:09:22.324216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = (\n",
    "    llm | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "\n",
    "events = [\n",
    "    event\n",
    "    async for event in chain.astream_events(\n",
    "        \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "        'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "        \"Each country should have the key `name` and `population`\",\n",
    "        version=\"v2\"\n",
    "    )\n",
    "]"
   ],
   "id": "10111535b6b358ab",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:09:48.691805Z",
     "start_time": "2025-03-16T14:09:48.682081Z"
    }
   },
   "cell_type": "code",
   "source": "events[:3]",
   "id": "324d13e15cdb7bc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chain_start',\n",
       "  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'},\n",
       "  'name': 'RunnableSequence',\n",
       "  'tags': [],\n",
       "  'run_id': '639575d4-b766-4b70-90fc-f8d2060bfc96',\n",
       "  'metadata': {},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_start',\n",
       "  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:1'],\n",
       "  'run_id': '1edff54b-39db-4819-81b7-1802f03f50c2',\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gemma-3-4b-it',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7,\n",
       "   'ls_max_tokens': 8064},\n",
       "  'parent_ids': ['639575d4-b766-4b70-90fc-f8d2060bfc96']},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'data': {'chunk': AIMessageChunk(content='```', additional_kwargs={}, response_metadata={}, id='run-1edff54b-39db-4819-81b7-1802f03f50c2')},\n",
       "  'run_id': '1edff54b-39db-4819-81b7-1802f03f50c2',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:1'],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gemma-3-4b-it',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7,\n",
       "   'ls_max_tokens': 8064},\n",
       "  'parent_ids': ['639575d4-b766-4b70-90fc-f8d2060bfc96']}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 输出模型和解析器的流事件。我们忽略了开始事件、结束事件和链中的事件。",
   "id": "dea0ce157ab88906"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:11:45.307941Z",
     "start_time": "2025-03-16T14:11:43.941880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\"\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ],
   "id": "b6a32d15c2ee52c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: '```'\n",
      "Chat model chunk: 'json'\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '{'\n",
      "Parser chunk: {}\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '  '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'countries'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' ['\n",
      "Parser chunk: {'countries': []}\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '    '\n",
      "Chat model chunk: '{'\n",
      "Parser chunk: {'countries': [{}]}\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '      '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'name'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' \"'\n",
      "Parser chunk: {'countries': [{'name': ''}]}\n",
      "Chat model chunk: 'France'\n",
      "...\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 因为模型和解析器都支持流式处理，所以我们能实时从两个组件中看到流式事件",
   "id": "cc7f65acd5dd38bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Filtering Events  过滤事件\n",
    "- 此 API 会产生大量的事件，因此能够过滤事件是有用的\n",
    "- 可以通过组件 name ，组件 tags 或组件 type 进行过滤。"
   ],
   "id": "5e2af873cc98b8ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 按Name",
   "id": "64a5ff40a5de3310"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:15:34.140765Z",
     "start_time": "2025-03-16T14:15:32.442972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = llm.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    include_names=[\"my_parser\"],\n",
    "    version=\"v2\"\n",
    "):\n",
    "    print(f'==> {max_events+1}: {event}')\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ],
   "id": "e9b30f5cf3dfbd6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 1: {'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'metadata': {}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 2: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 3: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 4: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 5: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': ''}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 6: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 7: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 6}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 8: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 9: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 679}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 10: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 6795}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "==> 11: {'event': 'on_parser_stream', 'run_id': '534a4201-b9e8-46e8-a8f8-69fd61453984', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 67950}]}}, 'parent_ids': ['8957b3bf-2e6a-473a-b37f-ce7f7eba2c4d']}\n",
      "...\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 按Type",
   "id": "1c86647aeb682802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:17:06.069110Z",
     "start_time": "2025-03-16T14:17:04.986878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = llm.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    include_types=[\"chat_model\"],\n",
    "    version=\"v2\"\n",
    "):\n",
    "    print(f'==> {max_events+1}: {event}')\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ],
   "id": "ce3c387a770613ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 1: {'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 2: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='```', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 3: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='json', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 4: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 5: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 6: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 7: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='  ', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 8: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\"', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 9: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='countries', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 10: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\":', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "==> 11: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' [', additional_kwargs={}, response_metadata={}, id='run-847e79fb-db8f-43ea-8bd4-677847cd21d4')}, 'run_id': '847e79fb-db8f-43ea-8bd4-677847cd21d4', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5431be96-7d4c-4682-b872-142f883a191c']}\n",
      "...\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 按Tags\n",
    "- 标签会被给定可运行组件的子组件继承。"
   ],
   "id": "6db20922624e2e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:18:22.663384Z",
     "start_time": "2025-03-16T14:18:21.680991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = (llm | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    include_tags=[\"my_chain\"],\n",
    "    version=\"v2\"\n",
    "):\n",
    "    print(f'==> {max_events+1}: {event}')\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ],
   "id": "857d8fdcf062ef96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> 1: {'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '5f427cde-6fd1-4a76-8b31-0f02c175eb78', 'metadata': {}, 'parent_ids': []}\n",
      "==> 2: {'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'run_id': 'ee90f991-4c98-4441-9957-ed0f79ba5d22', 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 3: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='```', additional_kwargs={}, response_metadata={}, id='run-ee90f991-4c98-4441-9957-ed0f79ba5d22')}, 'run_id': 'ee90f991-4c98-4441-9957-ed0f79ba5d22', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 4: {'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '80e38d98-d1bf-49f8-bdaa-1502a7e53b83', 'metadata': {}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 5: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='json', additional_kwargs={}, response_metadata={}, id='run-ee90f991-4c98-4441-9957-ed0f79ba5d22')}, 'run_id': 'ee90f991-4c98-4441-9957-ed0f79ba5d22', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 6: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={}, id='run-ee90f991-4c98-4441-9957-ed0f79ba5d22')}, 'run_id': 'ee90f991-4c98-4441-9957-ed0f79ba5d22', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 7: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='{', additional_kwargs={}, response_metadata={}, id='run-ee90f991-4c98-4441-9957-ed0f79ba5d22')}, 'run_id': 'ee90f991-4c98-4441-9957-ed0f79ba5d22', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 8: {'event': 'on_parser_stream', 'run_id': '80e38d98-d1bf-49f8-bdaa-1502a7e53b83', 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 9: {'event': 'on_chain_stream', 'run_id': '5f427cde-6fd1-4a76-8b31-0f02c175eb78', 'name': 'RunnableSequence', 'tags': ['my_chain'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': []}\n",
      "==> 10: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='\\n', additional_kwargs={}, response_metadata={}, id='run-ee90f991-4c98-4441-9957-ed0f79ba5d22')}, 'run_id': 'ee90f991-4c98-4441-9957-ed0f79ba5d22', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "==> 11: {'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='  ', additional_kwargs={}, response_metadata={}, id='run-ee90f991-4c98-4441-9957-ed0f79ba5d22')}, 'run_id': 'ee90f991-4c98-4441-9957-ed0f79ba5d22', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gemma-3-4b-it', 'ls_model_type': 'chat', 'ls_temperature': 0.7, 'ls_max_tokens': 8064}, 'parent_ids': ['5f427cde-6fd1-4a76-8b31-0f02c175eb78']}\n",
      "...\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Non-streaming components  非流式组件\n",
    "- 尽管此类组件在使用 astream 时可能会中断最终输出的流式传输，\n",
    "- 但 astream_events 仍然会从支持流式传输的中间步骤生成流式事件！"
   ],
   "id": "d9a8d7385c8aa73f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:19:59.581285Z",
     "start_time": "2025-03-16T14:19:59.566258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function that does not support streaming.\n",
    "# It operates on the finalizes inputs rather than\n",
    "# operating on the input stream.\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = (\n",
    "    llm | JsonOutputParser() | _extract_country_names\n",
    ")  # This parser only works with OpenAI right now"
   ],
   "id": "89326bbfc6cc6447",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:20:28.719814Z",
     "start_time": "2025-03-16T14:20:25.263123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 非流式\n",
    "async for chunk in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(chunk, flush=True)"
   ],
   "id": "3331da9e4126e029",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:20:56.997043Z",
     "start_time": "2025-03-16T14:20:55.637591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 流式\n",
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\"\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ],
   "id": "e3e04f262fd68243",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: '```'\n",
      "Chat model chunk: 'json'\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '{'\n",
      "Parser chunk: {}\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '  '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'countries'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' ['\n",
      "Parser chunk: {'countries': []}\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '    '\n",
      "Chat model chunk: '{'\n",
      "Parser chunk: {'countries': [{}]}\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '      '\n",
      "Chat model chunk: '\"'\n",
      "Chat model chunk: 'name'\n",
      "Chat model chunk: '\":'\n",
      "Chat model chunk: ' \"'\n",
      "Parser chunk: {'countries': [{'name': ''}]}\n",
      "Chat model chunk: 'France'\n",
      "Parser chunk: {'countries': [{'name': 'France'}]}\n",
      "Chat model chunk: '\",'\n",
      "Chat model chunk: '\\n'\n",
      "...\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Propagating Callbacks  传播回调\n",
    "- 如果您在工具中使用了运行可执行程序，请确保将回调传递给runnable；否则，将不会生成流事件。\n",
    "- 当使用 RunnableLambdas 或 @chain 装饰器时，回调会在后台自动传递。"
   ],
   "id": "6eefd74580701653"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:22:57.031003Z",
     "start_time": "2025-03-16T14:22:57.016412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "def reverse_word(word: str):\n",
    "    return word[::-1]\n",
    "\n",
    "# 回调会在后台自动传递\n",
    "reverse_word = RunnableLambda(reverse_word)\n",
    "\n",
    "\n",
    "@tool\n",
    "def bad_tool(word: str):\n",
    "    \"\"\"Custom tool that doesn't propagate callbacks.\"\"\"\n",
    "    return reverse_word.invoke(word)\n",
    "\n",
    "\n",
    "async for event in bad_tool.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ],
   "id": "6da6cd2847e4500",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'c6359add-5ea5-45dd-9853-b6560897cd24', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '0b07a90d-7170-436e-82c9-3ddf721b3c53', 'metadata': {}, 'parent_ids': ['c6359add-5ea5-45dd-9853-b6560897cd24']}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '0b07a90d-7170-436e-82c9-3ddf721b3c53', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['c6359add-5ea5-45dd-9853-b6560897cd24']}\n",
      "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'c6359add-5ea5-45dd-9853-b6560897cd24', 'name': 'bad_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 重新实现，现在我们可以从 reverse_word 可运行任务中接收到事件了。",
   "id": "a834b7ee06786a03"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:24:01.853948Z",
     "start_time": "2025-03-16T14:24:01.813004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@tool\n",
    "def correct_tool(word: str, callbacks):\n",
    "    \"\"\"A tool that correctly propagates callbacks.\"\"\"\n",
    "    return reverse_word.invoke(word, {\"callbacks\": callbacks}) # 手动回调\n",
    "\n",
    "\n",
    "async for event in correct_tool.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ],
   "id": "e8288922044507a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': 'bb418a24-1925-49eb-8ea1-38ae482ef7a2', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '9b49525f-50ec-4cce-8b0d-ec5c4af676d8', 'metadata': {}, 'parent_ids': ['bb418a24-1925-49eb-8ea1-38ae482ef7a2']}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '9b49525f-50ec-4cce-8b0d-ec5c4af676d8', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['bb418a24-1925-49eb-8ea1-38ae482ef7a2']}\n",
      "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'bb418a24-1925-49eb-8ea1-38ae482ef7a2', 'name': 'correct_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 在 Runnable Lambda 或 @chains 中调用可运行任务，那么回调将会自动传递给你。",
   "id": "4664b29ed2c5d637"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:26:07.153228Z",
     "start_time": "2025-03-16T14:26:07.135422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "reverse_and_double = RunnableLambda(reverse_and_double)\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v2\"):\n",
    "    print(event)"
   ],
   "id": "d372fb269e775828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': '4a049856-e9df-4967-a34f-2342c676d85e', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'f838b6d2-45ce-4efd-8693-82bcf9af1402', 'metadata': {}, 'parent_ids': ['4a049856-e9df-4967-a34f-2342c676d85e']}\n",
      "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': 'f838b6d2-45ce-4efd-8693-82bcf9af1402', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['4a049856-e9df-4967-a34f-2342c676d85e']}\n",
      "{'event': 'on_chain_stream', 'run_id': '4a049856-e9df-4967-a34f-2342c676d85e', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': '4a049856-e9df-4967-a34f-2342c676d85e', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- @chain 装饰器:",
   "id": "4530643ef16dd288"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T14:26:47.811897Z",
     "start_time": "2025-03-16T14:26:47.799949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v2\"):\n",
    "    print(event)"
   ],
   "id": "74266161be2fab22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': 'a36829b1-1ab8-4f06-be4a-3de6fbb8bfa0', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': '8dd948c7-b968-41e5-ae90-3ea4f2888588', 'metadata': {}, 'parent_ids': ['a36829b1-1ab8-4f06-be4a-3de6fbb8bfa0']}\n",
      "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': '8dd948c7-b968-41e5-ae90-3ea4f2888588', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['a36829b1-1ab8-4f06-be4a-3de6fbb8bfa0']}\n",
      "{'event': 'on_chain_stream', 'run_id': 'a36829b1-1ab8-4f06-be4a-3de6fbb8bfa0', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': 'a36829b1-1ab8-4f06-be4a-3de6fbb8bfa0', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f04420227dc0d141"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
