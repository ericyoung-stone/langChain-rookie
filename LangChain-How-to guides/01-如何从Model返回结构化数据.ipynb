{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to return structured data from a model\n",
    "- chat models\n",
    "- function/tool calling\n",
    "- structured output\n",
    "\n",
    "\n",
    "参考: [How to return structured data from a model](https://python.langchain.com/docs/how_to/structured_output/)"
   ],
   "id": "e7918bfcf96be661"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## chat models",
   "id": "e9c63d079d309dfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:04:34.465652Z",
     "start_time": "2025-03-16T03:04:34.451649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('/Users/ericyoung/ysx/code/github-study/langChain-rookie')\n",
    "from env_utils import load_environment_variables\n",
    "\n",
    "load_environment_variables()"
   ],
   "id": "e04acf8b6249bb5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__file__: /Users/ericyoung/ysx/code/github-study/langChain-rookie/env_utils.py\n",
      "dotenv_path1: /Users/ericyoung/ysx/code/github-study/langChain-rookie/.env\n",
      "load env ok\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 支持tool的 chat model",
   "id": "4d69472a833612b7"
  },
  {
   "cell_type": "code",
   "id": "5388aa50-da7b-47af-9141-a295fafe1975",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-03-16T03:04:39.180241Z",
     "start_time": "2025-03-16T03:04:38.683485Z"
    }
   },
   "source": [
    "# Ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:latest\", # gemma3:latest(不支持tool)\n",
    "    temperature=0.7,\n",
    "    # other params...\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 不支持tool的chat model",
   "id": "8bcc565d1b43f814"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:04:51.898616Z",
     "start_time": "2025-03-16T03:04:47.163264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 任何chat model\n",
    "from models import MyOpenAIModel\n",
    "\n",
    "model = MyOpenAIModel()\n",
    "response = model.generate(\"你好，介绍一下你自己\")\n",
    "print(response)"
   ],
   "id": "31ac354122cdeffd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您好！我是 Gemma，一个由 Google DeepMind 训练的大型语言模型。我是一个开放权重的 AI 助手，这意味着我的模型权重是公开的，您可以自由地使用和修改我。\n",
      "\n",
      "我可以接收文本和图像作为输入，并仅输出文本。 我可以尝试回答你的问题、提供信息、进行创作等等。\n",
      "\n",
      "由于我是由 Google DeepMind 训练的，因此我的知识截止到 2023 年初。\n",
      "\n",
      "希望您喜欢与我互动！ 您想聊些什么呢？\n",
      "您好！我是 Gemma，一个由 Google DeepMind 训练的大型语言模型。我是一个开放权重的 AI 助手，这意味着我的模型权重是公开的，您可以自由地使用和修改我。\n",
      "\n",
      "我可以接收文本和图像作为输入，并仅输出文本。 我可以尝试回答你的问题、提供信息、进行创作等等。\n",
      "\n",
      "由于我是由 Google DeepMind 训练的，因此我的知识截止到 2023 年初。\n",
      "\n",
      "希望您喜欢与我互动！ 您想聊些什么呢？\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ⭐️ 利用 ChatOpenAI 使用自己的base_url (模型本身不支持tool)\n",
    "- 支持结构化输出, with_structured_output方法\n",
    "- 支持使用工具, bind_tools方法\n",
    "- llm = ChatOpenAI(\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=0,\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "                # api_key=\"...\",\n",
    "                # base_url=\"...\",\n",
    "                # organization=\"...\",\n",
    "                # other params...\n",
    "            )"
   ],
   "id": "c5f1c53aef76d8bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:05:05.403799Z",
     "start_time": "2025-03-16T03:05:02.836322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models import get_base_url_model_with_tools\n",
    "llm2 = get_base_url_model_with_tools()\n",
    "print(llm2.invoke(\"你是谁?\").content)"
   ],
   "id": "4b823fdd30da3025",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是Gemma，一个开放权重的AI助手。我是一个由Google DeepMind训练的大型语言模型。\n",
      "\n",
      "我是一个由文本和图像组成的模型，主要提供文本输入和输出。 \n",
      "\n",
      "你可以把我当作一个工具来使用，我可以尝试回答你的问题、生成创意内容等等。\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:11:55.513370Z",
     "start_time": "2025-03-16T03:11:53.954397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def greeting(name: str):\n",
    "    '''向朋友致欢迎语'''\n",
    "    return f\"你好啊, {name}\"\n",
    "\n",
    "llm_with_tools = llm2.bind_tools([greeting])\n",
    "llm_with_tools.invoke(\"你好，我叫tom,希望可以成为你的新朋友\")"
   ],
   "id": "94b837e8060ea03a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '875086986', 'function': {'arguments': '{\"name\":\"tom\"}', 'name': 'greeting'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 390, 'total_tokens': 420, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma-3-4b-it', 'system_fingerprint': 'gemma-3-4b-it', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-877a6d31-646c-4766-a011-1d4121f510ea-0', tool_calls=[{'name': 'greeting', 'args': {'name': 'tom'}, 'id': '875086986', 'type': 'tool_call'}], usage_metadata={'input_tokens': 390, 'output_tokens': 30, 'total_tokens': 420, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## .with_structured_output() 结构化输出\n",
    "- 前置条件: 需要**支持结构化方法的模型**才可以,见 https://python.langchain.com/docs/integrations/chat/ ;\n",
    "- 例如, MyOpenAIModel 就无法支持, ChatOllama是支持的"
   ],
   "id": "edbef500d5f69421"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用Pydantic 进行结构化输出",
   "id": "584541e40f0d878"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T03:05:32.385730Z",
     "start_time": "2025-03-16T03:05:29.198847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pydantic class\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm2 = llm2.with_structured_output(Joke)\n",
    "\n",
    "structured_llm2.invoke(\"Tell me a joke about women and answer in chinese\")"
   ],
   "id": "a972d1f07d978e63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the woman break up with the mathematician?', punchline='Because he kept trying to find her area!', rating=3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 练习",
   "id": "c177627012582186"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:06:21.760300Z",
     "start_time": "2025-03-15T14:06:16.826254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class JenkinsJob(BaseModel):\n",
    "    \"\"\"build a Jenkins job info\"\"\"\n",
    "    job_name: str = Field(description=\"The name of the job\")\n",
    "    run_result: str = Field(description=\"The result of the job\")\n",
    "    build_number: Optional[str] = Field(description=\"The build number of the job, such as #1104\")\n",
    "\n",
    "structured_llm2 = llm.with_structured_output(JenkinsJob)\n",
    "\n",
    "structured_llm2.invoke(\"build a jenkins ci job and answer in chinese\")"
   ],
   "id": "a3b2654a729aecfc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JenkinsJob(job_name='example-job', run_result='SUCCESS', build_number='#1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 使用 TypedDict 进行结构化输出\n",
    "- Requirements\n",
    "    - Core: langchain-core>=0.2.26\n",
    "    - Typing 扩展：强烈建议从 typing_extensions 导入 Annotated 和 TypedDict ，而不是 typing ，以确保在不同版本的 Python 中行为一致。"
   ],
   "id": "d3ec8eacf0b8edc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:06:24.900390Z",
     "start_time": "2025-03-15T14:06:21.808094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Optional\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# TypedDict\n",
    "class Joke(TypedDict):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "\n",
    "    # Alternatively, we could have specified setup as:\n",
    "\n",
    "    # setup: str                    # no default, no description\n",
    "    # setup: Annotated[str, ...]    # no default, no description\n",
    "    # setup: Annotated[str, \"foo\"]  # default, no description\n",
    "\n",
    "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "id": "efd6a6b52fb15a19",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'punchline': 'Because it loved to be a purr-fessional!',\n",
       " 'rating': 8,\n",
       " 'setup': 'Why did the cat join the rock band?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 使用json schema进行结构化输出",
   "id": "83c7438a9a3be75d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:06:28.094066Z",
     "start_time": "2025-03-15T14:06:24.944109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_schema = {\n",
    "    \"title\": \"joke\",\n",
    "    \"description\": \"Joke to tell user.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup of the joke\",\n",
    "        },\n",
    "        \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline to the joke\",\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
    "            \"default\": None,\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"setup\", \"punchline\"],\n",
    "}\n",
    "structured_llm = llm.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "id": "5e7c3ddb3663d070",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'punchline': 'Because there are too many cheetahs!',\n",
       " 'rating': 8,\n",
       " 'setup': \"Why don't cats play poker in the jungle?\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### multiple schemas 进行结构化输出\n",
    "- 联合多个schemas,LLM通过用户的输入选择合适的schema进结构化输出\n",
    "- 支持Pydantic/TypedDict/json schema三种方式构造"
   ],
   "id": "4b9ff3545a0a071"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:08:30.257594Z",
     "start_time": "2025-03-15T14:08:26.729343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"A conversational response to the user's query\")\n",
    "\n",
    "\n",
    "class FinalResponse(BaseModel):\n",
    "    final_output: Union[Joke, ConversationalResponse]\n",
    "\n",
    "\n",
    "structured_llm2 = llm.with_structured_output(FinalResponse)\n",
    "\n",
    "structured_llm2.invoke(\"Tell me a joke about cats\")"
   ],
   "id": "4ee0fcda7d479241",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:08:53.330797Z",
     "start_time": "2025-03-15T14:08:50.810943Z"
    }
   },
   "cell_type": "code",
   "source": "structured_llm2.invoke(\"How are you today? must answer to me\")",
   "id": "4259840872437d2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## steaming 流式输出\n",
    "- 结构化输出模型的流式输出,尤其是TypedDict class or JSON Schema dict"
   ],
   "id": "b209a184d1d94a35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:18:54.148682Z",
     "start_time": "2025-03-15T14:18:49.530111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# TypedDict\n",
    "class Joke(TypedDict):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: Annotated[str, ..., \"The setup of the joke\"]\n",
    "    punchline: Annotated[str, ..., \"The punchline of the joke\"]\n",
    "    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "# 没有流式效果?????\n",
    "for chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n",
    "    print(chunk)"
   ],
   "id": "4541e3047940ab8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'punchline': 'Because they make up everything!', 'rating': 8, 'setup': \"Why don't scientists trust atoms?\"}\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Few-shot prompting\n",
    "- 对于更复杂的模式，向提示中添加少量示例非常有用"
   ],
   "id": "82e7e83d470a518b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:23:53.137501Z",
     "start_time": "2025-03-15T14:23:47.067425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\"), and rating.\n",
    "\n",
    "Here are some examples of jokes:\n",
    "\n",
    "example_user: Tell me a joke about planes\n",
    "example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n",
    "\n",
    "example_user: Tell me another joke about planes\n",
    "example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n",
    "\n",
    "example_user: Now about caterpillars\n",
    "example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n",
    "\n",
    "few_shot_structured_llm = prompt | structured_llm\n",
    "few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")"
   ],
   "id": "94dfc8bb93777fb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Woodpecker', punchline=\"Woodpecker drills holes in trees, but his wife always says 'hole-y woodpecker, poor bird!'\", rating=8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### few-shot的结构化输出通过工具调用\n",
    "- 当底层方法用于**结构化输出是通过工具调用时**，我们可以将示例作为显式的工具调用传递进去。\n",
    "- 你可以检查你使用的模型在其 API 参考中是否使用了工具调用。\n",
    "- 详见: https://python.langchain.com/docs/how_to/tools_few_shot/\n",
    "- 运行报错????"
   ],
   "id": "6ea859befc7f6e20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:32:14.495176Z",
     "start_time": "2025-03-15T14:32:14.478094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "examples = [\n",
    "    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"joke\",\n",
    "                \"args\": {\n",
    "                    \"setup\": \"Why don't planes ever get tired?\",\n",
    "                    \"punchline\": \"Because they have rest wings!\",\n",
    "                    \"rating\": 2,\n",
    "                },\n",
    "                \"id\": \"1\",\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.\n",
    "    ToolMessage(\"\", tool_call_id=\"1\"),\n",
    "    # Some models also expect an AIMessage to follow any ToolMessages,\n",
    "    # so you may need to add an AIMessage here.\n",
    "    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"joke\",\n",
    "                \"args\": {\n",
    "                    \"setup\": \"Cargo\",\n",
    "                    \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",\n",
    "                    \"rating\": 10,\n",
    "                },\n",
    "                \"id\": \"2\",\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(\"\", tool_call_id=\"2\"),\n",
    "    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"joke\",\n",
    "                \"args\": {\n",
    "                    \"setup\": \"Caterpillar\",\n",
    "                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",\n",
    "                    \"rating\": 5,\n",
    "                },\n",
    "                \"id\": \"3\",\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(\"\", tool_call_id=\"3\"),\n",
    "]\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") \\\n",
    "and the final punchline (the response to \"<setup> who?\").\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")]\n",
    ")\n",
    "few_shot_structured_llm = prompt | structured_llm\n",
    "few_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})"
   ],
   "id": "ec9d82d1e50ca042",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## (高级用法) 指定输出结构化的方法\n",
    "- 对于支持多种输出结构化方式的模型（即，它们**同时支持工具调用和 JSON 模式**），可以通过 method= 参数指定使用哪种方法"
   ],
   "id": "5c47f49aef079380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:39:22.708941Z",
     "start_time": "2025-03-15T14:39:18.290751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置json模式\n",
    "structured_llm = llm.with_structured_output(None, method=\"json_mode\")\n",
    "\n",
    "# 指定输出哪些key\n",
    "structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` , `rating` keys\"\n",
    ")"
   ],
   "id": "4d0809b14b703d96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't cats play poker in the jungle?\",\n",
       " 'punchline': 'Because there are too many cheetahs!',\n",
       " 'rating': 'G'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## (高级用法) Raw outputs  原始输出\n",
    "- LLMs 并不擅长生成结构化输出的，尤其是当模式变得复杂时。\n",
    "- 你可以通过传递 include_raw=True 来避免抛出异常并自行处理原始输出。\n",
    "- 输出的response中包含 原始消息输出(raw)、 解析成功的结果(parsed) , 解析错误的信息(parsing_error)："
   ],
   "id": "233bb57e29cc0164"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:47:49.347795Z",
     "start_time": "2025-03-15T14:47:44.325034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_llm = llm.with_structured_output(Joke, include_raw=True)\n",
    "\n",
    "res = structured_llm.invoke(\"Tell me a joke about cats\")\n",
    "print(res)\n",
    "print(\"\\nraw====\\n\")\n",
    "print(res['raw'])\n",
    "print(\"\\nparsed====\\n\")\n",
    "print(res['parsed'])\n",
    "print(\"\\nparsing_error====\\n\")\n",
    "print(res['parsing_error'])"
   ],
   "id": "5a6f576d6cc1bf6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw': AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-03-15T14:47:49.315938Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4955127084, 'load_duration': 44055584, 'prompt_eval_count': 204, 'prompt_eval_duration': 2522000000, 'eval_count': 50, 'eval_duration': 2383000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-cadc5235-c910-4cfc-b4dc-21a435901627-0', tool_calls=[{'name': 'Joke', 'args': {'punchline': 'Because there are too many cheetahs!', 'rating': 8, 'setup': \"Why don't cats play poker in the jungle?\"}, 'id': '278bad2b-6217-474b-ac43-ebceec97d7fb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 204, 'output_tokens': 50, 'total_tokens': 254}), 'parsed': Joke(setup=\"Why don't cats play poker in the jungle?\", punchline='Because there are too many cheetahs!', rating=8), 'parsing_error': None}\n",
      "\n",
      "raw====\n",
      "\n",
      "content='' additional_kwargs={} response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-03-15T14:47:49.315938Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4955127084, 'load_duration': 44055584, 'prompt_eval_count': 204, 'prompt_eval_duration': 2522000000, 'eval_count': 50, 'eval_duration': 2383000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-cadc5235-c910-4cfc-b4dc-21a435901627-0' tool_calls=[{'name': 'Joke', 'args': {'punchline': 'Because there are too many cheetahs!', 'rating': 8, 'setup': \"Why don't cats play poker in the jungle?\"}, 'id': '278bad2b-6217-474b-ac43-ebceec97d7fb', 'type': 'tool_call'}] usage_metadata={'input_tokens': 204, 'output_tokens': 50, 'total_tokens': 254}\n",
      "\n",
      "parsed====\n",
      "\n",
      "setup=\"Why don't cats play poker in the jungle?\" punchline='Because there are too many cheetahs!' rating=8\n",
      "\n",
      "parsing_error====\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ⭐️(高级用法) 不支持tool调用的模型解析模型输出\n",
    "- 直接提示并解析模型输出\n",
    "- 并非所有模型都支持 .with_structured_output() ，因为并非所有模型都具有工具调用或 JSON 模式支持。\n",
    "- 对于此类模型，您需要**直接提示模型使用特定格式，并使用输出解析器从原始模型输出中提取结构化响应**。"
   ],
   "id": "2028aeb549cc2bd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 使用 PydanticOutputParser\n",
    "- 以下示例使用内置的 PydanticOutputParser 来解析由提示匹配给定 Pydantic 模式的聊天模型输出。\n",
    "- 请注意，我们直接从解析器的方法中向提示添加了 format_instructions\n",
    "- 输出解析器与提示技术生成结构化输出的深入介绍, 参考: https://python.langchain.com/docs/how_to/output_parser_structured/"
   ],
   "id": "c83f697387db0df5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:54:39.948280Z",
     "start_time": "2025-03-15T14:54:39.932011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt.invoke({\"query\": query}).to_string())"
   ],
   "id": "17cd61d2fdb8c6b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer the user query. Wrap the output in `json` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of the person expressed in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
      "```\n",
      "Human: Anna is 23 years old and she is 6 feet tall\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T15:09:00.549015Z",
     "start_time": "2025-03-15T15:08:58.534981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ],
   "id": "e95a23a95d623527",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "People(people=[Person(name='Anna', height_in_meters=1.8288)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Custom Parsing  自定义解析\n",
    "- 您还可以使用 LangChain Expression Language ([LCEL](https://python.langchain.com/docs/concepts/lcel/)) 创建自定义提示和解析器，并使用普通函数解析模型的输出"
   ],
   "id": "c2f41bdc1e4d6c9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T15:07:12.699976Z",
     "start_time": "2025-03-15T15:07:12.686071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Output your answer as JSON that  \"\n",
    "            r\"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"\n",
    "            r\"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(schema=People.model_json_schema())\n",
    "\n",
    "\n",
    "# Custom parser\n",
    "def extract_json(message: AIMessage) -> List[dict]:\n",
    "    r\"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted JSON strings.\n",
    "    \"\"\"\n",
    "    text = message.content\n",
    "    # Define the regular expression pattern to match JSON blocks\n",
    "    pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"\n",
    "\n",
    "    # Find all non-overlapping matches of the pattern in the string\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
    "    try:\n",
    "        return [json.loads(match.strip()) for match in matches]\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Failed to parse: {message}\")"
   ],
   "id": "e0ac53fed5460c12",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T15:12:18.776764Z",
     "start_time": "2025-03-15T15:12:14.895393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "res = chain.invoke({\"query\": query})\n",
    "print(\"\\nres====\\n\")\n",
    "print(res)\n",
    "print(\"\\nparsed====\\n\")\n",
    "parsed = extract_json(res)\n",
    "print(parsed)"
   ],
   "id": "d869dcbb9e20b73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "res====\n",
      "\n",
      "content='```json\\n{\\n  \"people\": [\\n    {\\n      \"name\": \"Anna\",\\n      \"height_in_meters\": 1.8288\\n    }\\n  ]\\n}\\n```' additional_kwargs={} response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-03-15T15:12:18.762255Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3836013458, 'load_duration': 42675333, 'prompt_eval_count': 242, 'prompt_eval_duration': 1906000000, 'eval_count': 40, 'eval_duration': 1880000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-1f1688ab-b26b-44c9-a8c1-410c29e9fb01-0' usage_metadata={'input_tokens': 242, 'output_tokens': 40, 'total_tokens': 282}\n",
      "\n",
      "parsed====\n",
      "\n",
      "[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "84e759aa9523e98e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
