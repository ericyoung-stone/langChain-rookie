{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e6d6e7-65f9-48f3-a8ec-f214fa8123cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "- 使用提示模板和聊天模型构建一个简单的LLM应用程序。\n",
    "- Build a simple LLM application with prompt templates and chat models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579f9f9-c48c-4268-a050-7a11427f127f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 配置langSminth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ae81f6-6462-484e-a35a-6ec7b5edab3c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-19T14:18:15.932Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in an interactive environment, using current directory instead.\n",
      "/Users/ericyoung/ysx/code/github-study/langChain-rookie/.env\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -qU \"langchain[openai]\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "try:\n",
    "    print(__file__)  # 检查 __file__ 是否存在, os.path.dirname() 可以返回指定路径的父目录\n",
    "    dotenv_path = os.path.join(os.path.dirname(__file__), '.env')\n",
    "except NameError:\n",
    "    print(\"Running in an interactive environment, using current directory instead.\")\n",
    "    # 使用 os.path.join() 和 .. 返回指定路径的父目录\n",
    "    dotenv_path = os.path.normpath(os.path.join(os.getcwd(), '..', '.env'))\n",
    "print(dotenv_path)\n",
    "load_dotenv(dotenv_path, override=True)\n",
    "\n",
    "# 设置环境变量（仅对当前 Python 进程及其子进程生效）\n",
    "# os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "# getpass.getpass(\"Enter API key for LangSmith: \")\n",
    "# os.environ[\"LANGSMITH_PROJECT\"] =  os.getenv(\"LANGSMITH_PROJECT\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 验证\n",
    "# print(os.getenv(\"LANGSMITH_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983794b3-c788-402a-bcb6-abec8405aea5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 配置模型\n",
    "- ChatModels 是 LangChain Runnables 的实例，这意味着它们暴露了一个标准接口来与它们交互。\n",
    "- 要简单地调用模型，我们可以将消息列表传递给 .invoke 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a0c478-7f50-4f2d-b03e-37101f395a85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='qwen2.5:latest', temperature=0.7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mChatOllama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcustom_get_token_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrate_limiter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrate_limiters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseRateLimiter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdisable_streaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tool_calling'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmirostat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmirostat_eta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmirostat_tau\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_ctx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_gpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_thread\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_predict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepeat_last_n\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrepeat_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtfs_z\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_k\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_alive\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbase_url\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Ollama chat model integration.\n",
       "\n",
       ".. dropdown:: Setup\n",
       "    :open:\n",
       "\n",
       "    Install ``langchain-ollama`` and download any models you want to use from ollama.\n",
       "\n",
       "    .. code-block:: bash\n",
       "\n",
       "        ollama pull mistral:v0.3\n",
       "        pip install -U langchain-ollama\n",
       "\n",
       "Key init args — completion params:\n",
       "    model: str\n",
       "        Name of Ollama model to use.\n",
       "    temperature: float\n",
       "        Sampling temperature. Ranges from 0.0 to 1.0.\n",
       "    num_predict: Optional[int]\n",
       "        Max number of tokens to generate.\n",
       "\n",
       "See full list of supported init args and their descriptions in the params section.\n",
       "\n",
       "Instantiate:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain_ollama import ChatOllama\n",
       "\n",
       "        llm = ChatOllama(\n",
       "            model = \"llama3\",\n",
       "            temperature = 0.8,\n",
       "            num_predict = 256,\n",
       "            # other params ...\n",
       "        )\n",
       "\n",
       "Invoke:\n",
       "    .. code-block:: python\n",
       "\n",
       "        messages = [\n",
       "            (\"system\", \"You are a helpful translator. Translate the user sentence to French.\"),\n",
       "            (\"human\", \"I love programming.\"),\n",
       "        ]\n",
       "        llm.invoke(messages)\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        AIMessage(content='J'adore le programmation. (Note: \"programming\" can also refer to the act of writing code, so if you meant that, I could translate it as \"J'adore programmer\". But since you didn\\'t specify, I assumed you were talking about the activity itself, which is what \"le programmation\" usually refers to.)', response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:37:50.182604Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 3576619666, 'load_duration': 788524916, 'prompt_eval_count': 32, 'prompt_eval_duration': 128125000, 'eval_count': 71, 'eval_duration': 2656556000}, id='run-ba48f958-6402-41a5-b461-5e250a4ebd36-0')\n",
       "\n",
       "Stream:\n",
       "    .. code-block:: python\n",
       "\n",
       "        messages = [\n",
       "            (\"human\", \"Return the words Hello World!\"),\n",
       "        ]\n",
       "        for chunk in llm.stream(messages):\n",
       "            print(chunk)\n",
       "\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        content='Hello' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
       "        content=' World' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
       "        content='!' id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
       "        content='' response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:39:42.274449Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 411875125, 'load_duration': 1898166, 'prompt_eval_count': 14, 'prompt_eval_duration': 297320000, 'eval_count': 4, 'eval_duration': 111099000} id='run-327ff5ad-45c8-49fe-965c-0a93982e9be1'\n",
       "\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        stream = llm.stream(messages)\n",
       "        full = next(stream)\n",
       "        for chunk in stream:\n",
       "            full += chunk\n",
       "        full\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        AIMessageChunk(content='Je adore le programmation.(Note: \"programmation\" is the formal way to say \"programming\" in French, but informally, people might use the phrase \"le développement logiciel\" or simply \"le code\")', response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:38:54.933154Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1977300042, 'load_duration': 1345709, 'prompt_eval_duration': 159343000, 'eval_count': 47, 'eval_duration': 1815123000}, id='run-3c81a3ed-3e79-4dd3-a796-04064d804890')\n",
       "\n",
       "Async:\n",
       "    .. code-block:: python\n",
       "\n",
       "        messages = [\n",
       "            (\"human\", \"Hello how are you!\"),\n",
       "        ]\n",
       "        await llm.ainvoke(messages)\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        AIMessage(content=\"Hi there! I'm just an AI, so I don't have feelings or emotions like humans do. But I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\", response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:52:08.165478Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 2138492875, 'load_duration': 1364000, 'prompt_eval_count': 10, 'prompt_eval_duration': 297081000, 'eval_count': 47, 'eval_duration': 1838524000}, id='run-29c510ae-49a4-4cdd-8f23-b972bfab1c49-0')\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        messages = [\n",
       "            (\"human\", \"Say hello world!\"),\n",
       "        ]\n",
       "        async for chunk in llm.astream(messages):\n",
       "            print(chunk.content)\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        HEL\n",
       "        LO\n",
       "        WORLD\n",
       "        !\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        messages = [\n",
       "            (\"human\", \"Say hello world!\"),\n",
       "            (\"human\",\"Say goodbye world!\")\n",
       "        ]\n",
       "        await llm.abatch(messages)\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        [AIMessage(content='HELLO, WORLD!', response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:55:07.315396Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1696745458, 'load_duration': 1505000, 'prompt_eval_count': 8, 'prompt_eval_duration': 111627000, 'eval_count': 6, 'eval_duration': 185181000}, id='run-da6c7562-e25a-4a44-987a-2c83cd8c2686-0'),\n",
       "        AIMessage(content=\"It's been a blast chatting with you! Say goodbye to the world for me, and don't forget to come back and visit us again soon!\", response_metadata={'model': 'llama3', 'created_at': '2024-07-04T03:55:07.018076Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1399391083, 'load_duration': 1187417, 'prompt_eval_count': 20, 'prompt_eval_duration': 230349000, 'eval_count': 31, 'eval_duration': 1166047000}, id='run-96cad530-6f3e-4cf9-86b4-e0f8abba4cdb-0')]\n",
       "\n",
       "JSON mode:\n",
       "    .. code-block:: python\n",
       "\n",
       "\n",
       "        json_llm = ChatOllama(format=\"json\")\n",
       "        messages = [\n",
       "            (\"human\", \"Return a query for the weather in a random location and time of day with two keys: location and time_of_day. Respond using JSON only.\"),\n",
       "        ]\n",
       "        llm.invoke(messages).content\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        '{\"location\": \"Pune, India\", \"time_of_day\": \"morning\"}'\n",
       "\n",
       "Tool Calling:\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain_ollama import ChatOllama\n",
       "        from pydantic import BaseModel, Field\n",
       "\n",
       "        class Multiply(BaseModel):\n",
       "            a: int = Field(..., description=\"First integer\")\n",
       "            b: int = Field(..., description=\"Second integer\")\n",
       "\n",
       "        ans = await chat.invoke(\"What is 45*67\")\n",
       "        ans.tool_calls\n",
       "\n",
       "    .. code-block:: python\n",
       "\n",
       "        [{'name': 'Multiply',\n",
       "        'args': {'a': 45, 'b': 67},\n",
       "        'id': '420c3f3b-df10-4188-945f-eb3abdb40622',\n",
       "        'type': 'tool_call'}]\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/anaconda3/envs/langChain/lib/python3.13/site-packages/langchain_ollama/chat_models.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "ChatOllama?\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:latest\",\n",
    "    temperature=0.7,\n",
    ") # qwen2.5:latest deepseek-r1:1.5b\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d08f5-9c92-481d-a15d-5091c07db57c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1f22b6-228d-4a20-b697-255fc4e17dce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-02-21T12:48:20.934506Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2958457125, 'load_duration': 34257750, 'prompt_eval_count': 22, 'prompt_eval_duration': 2776000000, 'eval_count': 4, 'eval_duration': 145000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-8e1743ba-6a6e-40d9-89ab-3094b7608e41-0', usage_metadata={'input_tokens': 22, 'output_tokens': 4, 'total_tokens': 26})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd19b8-7d12-441d-8343-d44c094ec888",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LangChain 也支持通过字符串或 OpenAI 格式输入聊天模型。以下内容等效："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32476e13-e014-4a22-b39c-f2c6bd8b3a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.', additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-02-21T12:48:22.455116Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1500252292, 'load_duration': 12240667, 'prompt_eval_count': 30, 'prompt_eval_duration': 171000000, 'eval_count': 29, 'eval_duration': 1315000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-9b6b57a5-d94e-4118-bec4-edc71c16d20a-0', usage_metadata={'input_tokens': 30, 'output_tokens': 29, 'total_tokens': 59})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25280b0f-fb74-47c4-921d-d0187b28ab52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.', additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-02-21T12:48:23.828733Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1357269000, 'load_duration': 9281417, 'prompt_eval_count': 30, 'prompt_eval_duration': 47000000, 'eval_count': 29, 'eval_duration': 1299000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-e0ca4da6-d851-4245-9e97-2b82c25aeddd-0', usage_metadata={'input_tokens': 30, 'output_tokens': 29, 'total_tokens': 59})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1affe62c-118e-40f9-95ae-fa714afda824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.', additional_kwargs={}, response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-02-21T12:48:25.207661Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1361383625, 'load_duration': 9080667, 'prompt_eval_count': 30, 'prompt_eval_duration': 47000000, 'eval_count': 29, 'eval_duration': 1304000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-a38d2cd3-68b5-4b2c-ad12-13b76946bfa9-0', usage_metadata={'input_tokens': 30, 'output_tokens': 29, 'total_tokens': 59})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(\"Hello\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6272191-6580-4ae9-afdd-b62855235574",
   "metadata": {},
   "source": [
    "# 流式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21ec1cd2-6db2-4e94-9efa-67c931183d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='C' additional_kwargs={} response_metadata={} id='run-4fbf5601-6655-4e95-8236-5819f5b22d04'|content='iao' additional_kwargs={} response_metadata={} id='run-4fbf5601-6655-4e95-8236-5819f5b22d04'|content='!' additional_kwargs={} response_metadata={} id='run-4fbf5601-6655-4e95-8236-5819f5b22d04'|content='' additional_kwargs={} response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-02-21T12:48:25.558112Z', 'done': True, 'done_reason': 'stop', 'total_duration': 327406292, 'load_duration': 21151250, 'prompt_eval_count': 22, 'prompt_eval_duration': 164000000, 'eval_count': 4, 'eval_duration': 140000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-4fbf5601-6655-4e95-8236-5819f5b22d04' usage_metadata={'input_tokens': 22, 'output_tokens': 4, 'total_tokens': 26}|"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531165a8-ec42-44ab-8615-a899ec61cbb2",
   "metadata": {},
   "source": [
    "# Prompt Templates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c15ccd1-9974-4f0e-b361-bbe0ae0fe5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26314d2-0780-4e63-95a3-c792ddead064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Chinese', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 变量字典\n",
    "prompt = prompt_template.invoke({\"language\": \"Chinese\", \"text\": \"hi!\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03c17d3-4775-4068-9be8-9faa0562b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following from English into Chinese', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064a6d08-5a1d-4565-8680-b72b1d0b98c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='你好！' additional_kwargs={} response_metadata={'model': 'qwen2.5:latest', 'created_at': '2025-02-21T12:48:26.01358Z', 'done': True, 'done_reason': 'stop', 'total_duration': 302999791, 'load_duration': 23945791, 'prompt_eval_count': 22, 'prompt_eval_duration': 176000000, 'eval_count': 3, 'eval_duration': 99000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-9b000bb1-348b-4598-85f1-b9d5f43d7b38-0' usage_metadata={'input_tokens': 22, 'output_tokens': 3, 'total_tokens': 25}\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6fd804-bc51-44ae-b6b2-480b8c6d5f4c",
   "metadata": {},
   "source": [
    "# 总结\n",
    "- 构造LLM对象 .invoke messages\n",
    "- 使用message对象:HumanMessage/SystemMessage\n",
    "- 使用prompt模板:ChatPromptTemplate .invoke 参数字典\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdae7eb-1965-4946-a4df-b92dd289f47e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
